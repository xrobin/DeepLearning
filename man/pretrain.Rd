% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/pretrain.R
\name{pretrain}
\alias{pretrain}
\alias{pretrain.DeepBeliefNet}
\alias{pretrain.RestrictedBolzmannMachine}
\title{Pre-trains the DeepBeliefNet or RestrictedBolzmannMachine}
\usage{
pretrain(x, data, ...)

\method{pretrain}{RestrictedBolzmannMachine}(x, data, miniters = 100,
  maxiters = floor(dim(data)[1]/batchsize), batchsize = 100, momentum = 0,
  penalization = c("l1", "l2", "none"), lambda = 0, lambda.b = lambda,
  lambda.c = lambda, lambda.W = lambda, epsilon = ifelse(x$output$type ==
  "gaussian", 0.001, 0.1), epsilon.b = epsilon, epsilon.c = epsilon,
  epsilon.W = epsilon, train.b = TRUE, train.c = TRUE,
  continue.function = continue.function.exponential,
  continue.function.frequency = 1000, continue.stop.limit = 30,
  diag = list(rate = diag.rate, data = diag.data, f = diag.function),
  diag.rate = c("none", "each", "accelerate"), diag.data = NULL,
  diag.function = NULL, n.proc = detectCores() - 1, ...)

\method{pretrain}{DeepBeliefNet}(x, data, miniters = 100,
  maxiters = floor(dim(data)[1]/batchsize), batchsize = 100,
  skip = numeric(0), momentum = 0, penalization = "l1", lambda = 2e-04,
  lambda.b = lambda, lambda.c = lambda, lambda.W = lambda,
  epsilon = 0.1, epsilon.b = epsilon, epsilon.c = epsilon,
  epsilon.W = epsilon, train.b = TRUE, train.c = length(x) - 1,
  continue.function = continue.function.exponential,
  continue.function.frequency = 100, continue.stop.limit = 3,
  diag = list(rate = diag.rate, data = diag.data, f = diag.function),
  diag.rate = c("none", "each", "accelerate"), diag.data = NULL,
  diag.function = NULL, n.proc = detectCores() - 1, ...)
}
\arguments{
\item{x}{the \code{\link{DeepBeliefNet}} or \code{\link{RestrictedBolzmannMachine}} object}

\item{data}{the dataset, either as matrix or data.frame. The number of columns must match the number of nodes of the network input}

\item{...}{ignored}

\item{miniters, maxiters}{minimum and maximum number of iterations to perform}

\item{batchsize}{the size of the minibatches}

\item{momentum}{the momentum, between 0 (no momentum) and 1 (no training). See the Momentums section below.}

\item{penalization}{the penalization mode. Either \dQuote{l1} (sparse), \dQuote{l2} (quadratic) or \dQuote{none}.}

\item{lambda}{penalty on large weights (weight-decay). Alternatively one can define \code{lambda.b}, \code{lambda.c} and \code{lambda.W} to constrain 
\code{b}s, \code{c}s and \code{W}s, respectively. Default: 0 = no penalization (equivalent to \code{penalization="none"}).}

\item{lambda.b, lambda.c, lambda.W}{separate penalty rates for \code{b}s, \code{c}s and \code{W}s. Take precedence over \code{lambda}.}

\item{epsilon}{learning rate. Alternatively one can define \code{epsilon.b}, \code{epsilon.c} and \code{epsilon.W} (see below)
to learn \code{b}s, \code{c}s and \code{W}s, respectively, at different speeds. Defaut: 0.1 (for layers where all inputs and outputs are binary or continuous)
 or 0.001 (for layers with gaussian input or output).}

\item{epsilon.b, epsilon.c, epsilon.W}{separate learning rates for \code{b}s, \code{c}s and \code{W}s. Take precedence over \code{epsilon}.}

\item{train.b, train.c}{whether (\code{\link{RestrictedBolzmannMachine}}) or on which layers (\code{\link{DeepBeliefNet}}) to update the \code{b}s and \code{c}s. For a \code{\link{RestrictedBolzmannMachine}}, must be a logical of length 1. For a \code{\link{DeepBeliefNet}} must be a logical (can be recycled) or numeric index of layers.}

\item{continue.function}{that can stop the pre-training between miniters and maxiters if it returns \code{FALSE}. 
By default, \code{\link{continue.function.exponential}} will be used. An alternative is to use \code{\link{continue.function.always}} that will always return true and thus carry on with the training until maxiters is reached.
A user-supplied function must accept \code{(error, iter, batchsize)} as input and return a \code{\link{logical}} of length 1. The training is stopped when it returns \code{FALSE}.}

\item{continue.function.frequency}{the frequency at which continue.function will be assessed.}

\item{continue.stop.limit}{the number of consecutive times \code{continue.function} must return \code{FALSE} before the training is stopped. For example, \code{1} will stop as soon as \code{continue.function} returns \code{FALSE}, whereas \code{Inf} will ensure the result of \code{continue.function} is never enforced (but the function is still executed). The default is \code{3} so the training will continue until 3 consecutive calls of \code{continue.function} returned \code{FALSE}, giving more robustness to the decision.}

\item{diag, diag.rate, diag.data, diag.function}{diagnostic specifications. See details.}

\item{n.proc}{number of cores to be used for Eigen computations}

\item{skip}{numeric vector of the RestrictedBolzmannMachine of the DeepBeliefNet to be skipped.}
}
\value{
pre-trained object with the \code{pretrained} switch set to \code{TRUE}.
}
\description{
A contrastive divergence method is used to train each layer sequentially.
}
\section{Pretraining Layers of the Deep Belief Net with Different Parameters}{

It is possible to pre-train the layers of a DeepBeliefNet with different parameters. The following parameters can be supplied as vectors with length of the network - 1:
\code{batchsize}, \code{penalization}, \code{labmda}, \code{lambda.b}, \code{lambda.c}, \code{lambda.W}, 
\code{epsilon}, \code{epsilon.b}, \code{epsilon.c} and \code{epsilon.W}.
The values will be recycled if necessary (with essentially no warning if the lengths doesn't match). The special case of the \code{momentum} parameters is described below.
}

\section{Momentums}{

 The \code{momentum} parameter can take several length, and will be interpreted accordingly:
\itemize{
\item \code{1}: constant momentum
\item \code{2}: a gradient, will be interpreted as seq(momentum[1], momentum[2], length.out=maxiters)
\item \code{maxiter}: encodes the momentum per iteration
}
To specify different \code{momentum}s for the different layers of a DeepBeliefNet, they must be passed as a \code{\link{list}} of the same length than the number
of RestrictedBolzmannMachines to pretrain,
and they will be interpreted per layer as described above.
}

\section{Diagnostic specifications}{

The specifications can be passed directly in a list with elements \code{rate}, \code{data} and \code{f}, or separately with parameters \code{diag.rate}, \code{diag.data} and \code{diag.function}. The function must be of the following form:
\code{function(rbm, batch, data, iter, batchsize, maxiters, layer)}
\itemize{
\item \code{rbm}: the RBM object after the training iteration.
\item \code{batch}: the batch that was used at that iteration.
\item \code{data}: the data provided in \code{diag.data} or \code{diag$data}, possibly transformed through the previous layers of the DBN.
\item \code{iter}: the training iteration number, starting from 0 (before the first iteration).
\item \code{batchsize}: the size of the batch.
\item \code{maxiters}: the target number of iterations.
\item \code{layer}: the layer number, starting from 0.
}

The following \code{diag.rate} or \code{diag$rate} are supported:
\itemize{
\item \dQuote{none}: the diag function will never be called.
\item \dQuote{each}: the diag function will be called before the first iteration, and at the end of each iteration.
\item \dQuote{accelerate}: the diag function will called before the first iteration, at the first 200 iterations, and then with a rate slowing down proportionally with the iteration number.
}

Note that diag functions incur a slight overhead as they involve a callback to R and multiple object conversions. Setting \code{diag.rate = "none"} removes any overhead.
}
\examples{
library(mnist)
data(mnist)
# Initialize a 784-1000-500-250-30 layers DBN to process the MNIST data set
dbn.mnist <- DeepBeliefNet(Layers(c(784, 1000, 500, 250, 30), input="continuous", output="gaussian"))
print(dbn.mnist)

\dontrun{
# Pre-train this DBN
pretrained.mnist <- pretrain(dbn.mnist, mnist$train$x, 
								 penalization = "l2", lambda=0.0002, epsilon=c(.1, .1, .1, .001), 
								 batchsize = 100, maxiters=1000000)
}

\dontrun{
# Pretrain with a progress bar
# In this case the overhead is around 1\%
diag <- list(rate = "accelerate", data = NULL, f = function(rbm, batch, data, iter, batchsize, maxiters, layer) {
	if (iter == 0) {
		DBNprogressBar <<- txtProgressBar(min = 0, max = maxiters, initial = 0, width = NA, style = 3)
	}
	else if (iter == maxiters) {
		setTxtProgressBar(DBNprogressBar, iter)
		close(DBNprogressBar)
	}
	else {
		setTxtProgressBar(DBNprogressBar, iter)
	}
})
pretrained.mnist <- pretrain(dbn.mnist, mnist$train$x,  penalization = "l2", lambda=0.0002,
                             epsilon=c(.1, .1, .1, .001), batchsize = 100, maxiters=1e4,
                             continue.function = continue.function.always, diag = diag)
}
}

