% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/train.R
\docType{data}
\name{train}
\alias{train}
\alias{train.progress}
\title{Fine-tunes the DeepBeliefNet}
\format{An object of class \code{list} of length 3.}
\usage{
train(x, data, miniters = 100, maxiters = 1000, batchsize = 100,
  optim.control = list(), continue.function = continue.function.exponential,
  continue.function.frequency = 100, continue.stop.limit = 3,
  diag = list(rate = diag.rate, data = diag.data, f = diag.function),
  diag.rate = c("none", "each", "accelerate"), diag.data = NULL,
  diag.function = NULL, n.proc = detectCores() - 1, ...)

train.progress
}
\arguments{
\item{x}{the DBN}

\item{data}{the training data}

\item{miniters, maxiters}{minimum and maximum number of iterations to perform}

\item{batchsize}{the size of the batches on which error & gradients are averaged}

\item{optim.control}{control arguments for the optim function that are not typically changed for normal operation. The parameters are:
maxit, type, trace, steplength, stepredn, acctol, reltest, abstol, intol, setstep. Their default values are defined in TrainParameters.h.}

\item{continue.function}{that can stop the training between miniters and maxiters if it returns \code{FALSE}. 
By default, \code{\link{continue.function.exponential}} will be used. An alternative is to use \code{\link{continue.function.always}} that will always return true and thus carry on with the training until maxiters is reached.
A user-supplied function must accept \code{(error, iter, batchsize)} as input and return a \code{\link{logical}} of length 1. The training is stopped when it returns \code{FALSE}.}

\item{continue.function.frequency}{the frequency at which continue.function will be assessed.}

\item{continue.stop.limit}{the number of consecutive times \code{continue.function} must return \code{FALSE} before the training is stopped. For example, \code{1} will stop as soon as \code{continue.function} returns \code{FALSE}, whereas \code{Inf} will ensure the result of \code{continue.function} is never enforced (but the function is still executed). The default is \code{3} so the training will continue until 3 consecutive calls of \code{continue.function} returned \code{FALSE}, giving more robustness to the decision.}

\item{diag, diag.rate, diag.data, diag.function}{diagnmostic specifications. See details.}

\item{n.proc}{number of cores to be used for Eigen computations}

\item{...}{ignored}
}
\value{
the fine-tuned DBN
}
\description{
Performs fine-tuning on the DBN network with backpropagation.
}
\section{Diagnostic specifications}{

The specifications can be passed directly in a list with elements \code{rate}, \code{data} and \code{f}, or separately with parameters \code{diag.rate}, \code{diag.data} and \code{diag.function}. The function must be of the following form:
\code{function(rbm, batch, data, iter, batchsize, maxiters)}
\itemize{
\item \code{rbm}: the RBM object after the training iteration.
\item \code{batch}: the batch that was used at that iteration.
\item \code{data}: the data provided in \code{diag.data} or \code{diag$data}.
\item \code{iter}: the training iteration number, starting from 0 (before the first iteration).
\item \code{batchsize}: the size of the batch.
\item \code{maxiters}: the target number of iterations.
}
Note the absence of the \code{layer} argument that is available only in \code{\link{pretrain}}.

The following \code{diag.rate} or \code{diag$rate} are supported:
\itemize{
\item \dQuote{none}: the diag function will never be called.
\item \dQuote{each}: the diag function will be called before the first iteration, and at the end of each iteration.
\item \dQuote{accelerate}: the diag function will called before the first iteration, at the first 200 iterations, and then with a rate slowing down proportionally with the iteration number. It is always called at the last iteration.
}

Note that diag functions incur a slight overhead as they involve a callback to R and multiple object conversions. Setting \code{diag.rate = "none"} removes any overhead.
}

\section{Progress}{

\code{train.progress} is a convenient pre-built diagnostic specification that displays a progress bar.
}
\examples{
data(pretrained.mnist)

\dontrun{
# Fine-tune the DBN with backpropagation
trained.mnist <- train(unroll(pretrained.mnist), mnist$train$x, maxiters = 2000, batchsize = 1000,
                       optim.control = list(maxit = 10))
}
\dontrun{
# Train with a progress bar
# In this case the overhead is nearly 0
diag <- list(rate = "each", data = NULL, f = function(rbm, batch, data, iter, batchsize, maxiters) {
	if (iter == 0) {
		DBNprogressBar <<- txtProgressBar(min = 0, max = maxiters, initial = 0, width = NA, style = 3)
	}
	else if (iter == maxiters) {
		setTxtProgressBar(DBNprogressBar, iter)
		close(DBNprogressBar)
	}
	else {
		setTxtProgressBar(DBNprogressBar, iter)
	}
})
trained.mnist <- train(unroll(pretrained.mnist), mnist$train$x, maxiters = 1000, batchsize = 100,
                       continue.function = continue.function.always, diag = diag)
# Equivalent to using train.progress
trained.mnist <- train(unroll(pretrained.mnist), mnist$train$x, maxiters = 1000, batchsize = 100,
                       continue.function = continue.function.always, diag = train.progress)
}
}
\keyword{datasets}

